---
title: "Problem Set 1 A-ERM"
author: "Carlos Gonzalez"
date: '2022-10-14'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, results='hide', message=FALSE}
# Libraries
library(readxl)
library(tidyverse)
```


## Question 1: Binary Choice

# Part a)

```{r}
set.seed(1234) # For replicability
```

# Part b)

There is around a 65\% chance of individual i buying the good

```{r}
# Parameters
n = 5000
beta = 1
alpha = -0.5
params_e = c(0,1)
params_x = c(1.5, 2)
params_model = c(alpha, beta)

# Generate data
gen_data = function(n, params_e, params_x, params_model){
  
  # Recover values from input
  location = params_e[1]
  scale = params_e[2]
  mean_x = params_x[1]
  sd_x = params_x[2]
  alpha = params_model[1]
  beta = params_model[2]
  
  data_binary = tibble(e_i = rlogis(n, location, scale),
                       x_i = rnorm(n, mean_x, sd_x),
                       u_star = alpha + beta*x_i + e_i, # Underliying var
                       u_i = ifelse(u_star > 0, 1, 0))  #Observed variable
  # Outside option is normalized to zero
  
  data_binary
  
}


# Average purchase probability
data_binary = gen_data(n, params_e, params_x, params_model)
prob_buying = mean(data_binary$u_i>0)
prob_buying

```

# Part c)

The Likelihood (L) of a particular observation in the binomial logistic model is characterized by P(u_i=1|X) = p_1 = exp(z) / (1 + exp(z)) = plogis(z) where z = alpha + beta*x_i. p_0 can be defined as 1-p_1

The Log-Likelihood (LL) of a particular realization of data is then simply the sum across i of log(p_1) * (indicator u_i = 1) + log(1-p_1)*(indicator u_i = 0)

Non-deterministic probabilities p_j != 0 for j in \{0,1\} are needed for identification.

```{r}

ll = function(params){
  
  # Let params be a 1x2 vector st
  alpha = params[1]
  beta = params[2]
  
  # Define probabilities given parametric assumptions
  p_1 = exp(alpha + beta*data_binary$x_i) / (1 +
    exp(alpha + beta*data_binary$x_i))

  # Characterize log-likelihood
 - sum(data_binary$u_i*log(p_1) + (1-data_binary$u_i)*log(1-p_1))
}

```

# Part d)

We observe that LL is minimized around beta = 1. This is fully compatible with our model, given that the DGP was characterized via beta = 1.

```{r, warning=FALSE}

# Create tibble for plotting
data_plot = tibble(aux_seq = seq(from = -1, to = 4, length.out = 100),
                   ll_values = c())

# Initialize index to complete values
t = 1
for (beta in data_plot$aux_seq){
  data_plot$ll_values[t] = ll(c(alpha, beta))
  t = t+1
}

# Plot
data_plot |> ggplot(mapping = aes(x = aux_seq, y = ll_values)) +
             geom_point()


```


# Part e) to g)

I use an approach slightly different to the one used in class. In particular, I derive the gradient for the LL, such that I can boost the performance of the optimization algorithm. Also, note that the code above was already robust to the inclusion of alpha, so no further modifications take place in this regard.

```{r}
ll_gradient<-function(params){
  alpha = params[1]
  beta = params[2]
  
  c(- sum(data_binary$u_i- (1 / (1 + exp(-(alpha + data_binary$x_i*beta))))),
  - sum((data_binary$u_i - 
         (1 / (1 + exp(-(alpha + data_binary$x_i*beta)))))*data_binary$x_i))
}

results = optim(par = c(0,0),
                 fn = ll,
                 gr = ll_gradient,
                 method = 'BFGS',
                 hessian = TRUE)

# MLE values
results$par

#MLE SE
sqrt(diag(solve(results$hessian)))

```
## Question 2

# Part a) and b)

I first notice that data is in wide format. According to standard practice I recode it as long.

```{r}
# Read data
multi = read_xlsx("../data/insurance_choice_data.xlsx")

# Initial exploration
# head(multi)

# Reshape
multi_clean = multi |> 
  pivot_longer(starts_with('price_'), names_to = 'product', values_to = 'price') |>
  mutate(product = as.numeric(gsub("price_", "", product)))

# ids are uniformly counted three times, thus, there is no impact on shares
total = nrow(multi_clean)
multi_clean |> group_by(choice) |> summarize("Share %" = n()/total)

```
# Part c) and d)

I understand that there is nothing left for us to do in part c). alpha_1 = 0 is necessary for regularization, as probabilities are invariant to nominal addition (i.e. P(alpha_0 + x_i > alpha_1 + x_i') = P(alpha_0 + delta + x_i > alpha_1 + delta + x_i')).

# Part e)

```{r}

ll_multi = function(theta){
  
  # Let theta be a 1x3 vector st
  alpha_2 = theta[1]
  alpha_3 = theta[2]
  beta = theta[3]
  
  alpha_all = c(0, alpha_2, alpha_3)

  # Prepare data for function
  multi_clean = multi_clean |> arrange(person_id, product) |>
                mutate(exp_zj = exp(alpha_all + beta*price),
                       d_ij = ifelse(choice == product, 1, 0))
  sum_expz = multi_clean |> group_by(person_id) |>
          summarise(sum_expz = sum(exp_zj))
  multi_clean = multi_clean |> left_join(sum_expz, by="person_id") |>
                mutate(p_ij = exp_zj / sum_expz)
  
  - sum(multi_clean$d_ij*log(multi_clean$p_ij))
}
```

# Part f)

```{r}
results = optim(c(0,0,0),
                fn = ll_multi,
                hessian = TRUE)
#MLE values
bench_values = results$par
bench_values

#SE
sqrt(diag(solve(results$hessian)))
```

# Part g)
They all seem to be negative. The fact that the beta coefficient representing price effects is negative is not too surprising, however, there seems to be a strong preference towards good one, whose alpha was set equal 0. This fact could be driven by unobserved differences driving up good 1 preferences (i.e. quality differences) or could be driven by other behavioral components in decision making like inattention.

## Question 3

```{r}
ll_inattention = function(theta){
  
  # Let theta be a 1x5 vector st
  alpha_2 = theta[1]
  alpha_3 = theta[2]
  beta = theta[3]
  w = theta[4]
  gamma = theta[5]
  
  alpha_all = c(0, alpha_2, alpha_3)

  # Prepare data for function
  multi_clean = multi_clean |>
                mutate(exp_zj = exp(alpha_all + beta*price),
                       d_ij = ifelse(choice == product, 1, 0))
  sum_expz = multi_clean |> group_by(person_id) |>
          summarise(sum_expz = sum(exp_zj))
  
  multi_clean = multi_clean |> left_join(sum_expz, by="person_id") |>
                mutate(p_ij_star = exp_zj / sum_expz,
                       mu = exp(w + gamma*price)/(1+exp(w + gamma*price)),
                       p_ij = ifelse(product==1, (1-mu) + mu*p_ij_star,
                                     mu*p_ij_star))         
  
  - sum(multi_clean$d_ij*log(multi_clean$p_ij))
}
```


```{r}
results = optim(c(0,0,0,0,0),
                fn = ll_inattention,
                hessian = TRUE)
#MLE values
inatt_values = results$par
w = inatt_values[4]
gamma = inatt_values[5]

#SE
se = sqrt(diag(solve(results$hessian)))

# Display
tibble("Parameter" = c("alpha_2", "alpha_3", "beta", "omega", "gamma"),
       "Benchmark Values" = c(bench_values, NA, NA),
       "Inattention Values" = inatt_values, "Standard Errors" = se)

#Finally, we recover mu
multi_clean = multi_clean|> mutate(mu = exp(w + gamma*price)/(1+exp(w + gamma*price)))

# Some descriptive stats about mu
summary(multi_clean$mu)
```

# Part d)

Although some product difference seems to remain, we observe a dramatic reduction of product fixed effects when we allow for inattention. Similarly, price seems to be a higher driver of consumer choices than originally identified. Finally, observe that mu plays a very high role for some consumers. In particular, this account for up to 98\% of consumer decision for some individuals and it represents over a 90\% of choice behavior for the 3rd quartile.
