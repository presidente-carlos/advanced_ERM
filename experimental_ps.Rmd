---
title: "Problem Set Experiments - AERM"
author: "Carlos Gonzalez"
date: '2022-12-04'
output: pdf_document
---

```{r setup, include=FALSE}
# tinytex::install_tinytex() #For knitting
```

## Question 1

```{r}
# Libraries
library(readxl) # To read data
library(dplyr) # For data manipulation
library(ggplot2) # For graphics
library(modelsummary) # For regression output
```

```{r}
# Load data and filtering
# Keep subjects who participated in the baseline treatment only
data = read_xlsx("../data/LiesInDisguise_alldata.xlsx")
baseline = data |> filter(Baseline == 1) 
```

```{r, warning=FALSE}
# Plotting
baseline |> group_by(Profit) |> 
          summarise(share = 100* n() / nrow(baseline)) |>
          ggplot() +
          geom_col(aes(x = Profit, y = share),
                   width = .5, color = "black", fill = "orange") +
          geom_text(aes(x = Profit, y = share, 
                        label=round(share, 2)),
                    position=position_dodge(width=0.9),
                    vjust=-0.25, size = 3) +
          geom_line(aes(x = Profit, y = 100/6)) + # Unif Distr
          theme_classic() +
          scale_x_discrete(limits = c(0:5)) +
          xlab("Reported Outcome") +
          ylab("Share %")
```

## Question 2

I have decided to run a Chi-Square goodness of fit test (CSGoFT). CSGoFT is a good alternative to Kolmogorov-Smirnov test for univariate discrete data. CSGoFT essentially computes a ratio between (the squared) difference of empirical bin sizes and theoretical bin sizes, and theoretical bin sizes. Although this test is not restricted to discrete data, it is a natural selection given that no artificial creation of bins need to be done in first place. In the paper, authors claim to run a Kolmogorov-Smirnov test (KST) against the theoretical distribution (a discrete uniform distribution). However, to the best of my knowledge, KST only operates for continuous variables. Approximations are not too bad when the predicted number of ties is very small (i.e. Unif[0, 1000]), but could suffer badly from small binned data like that generated by rolling a die, so I do not understand the theoretical guarantees of their analysis.

As we can see, the p-value of our CSGoFT is very close to zero, hence we can reject the null-hypothesis (data being generated by a uniform random distribution) at standard confidence levels.

```{r}
# Obtain counts for CSGoFT
baseline_counts = baseline |> group_by(Die) |> 
  summarise(number = n())

# CSGoFT
chisq.test(baseline_counts$number, p = rep(1/6, 6))
```

## Question 3

I am running a simple linear regression model on gender, age and number of participations for the individuals who participated in the baseline treatment. I acknowledge that linear regression might not be the most suitted tool for this analysis, and probably other regression designs like Poisson Regression could do better. However, I believe that linear regression provides with the most interpretable results, which is one of the key elements of our analysis.

Robust Standard errors are reported in parenthesis.

The variables of gender and age are included to detect possible heterogeneous lying patterns in data. We do not observe any significant effect of gender nor age on profit claim. If anything, men might be more likely to report higher profits than women (at 90\% confidence level).

Finally, the variable of number of participations is positive but not significant. This result is actually expected as all subjects did the baseline test first, so there is no possible expertise gain from repeated interaction.

Overall, our model has a very low F-statistic what suggest that none of our variables might jointly explain the variability in the dependent variable.


```{r}
regression = lm(Profit ~ GenderF + Age +
                      NumParticipations,
                data = baseline)
modelsummary(models = list("Profit" = regression),
             stars = T,
             gof_omit = 'Log.Lik|R2 Adj.|AIC|BIC|RMSE')
```


